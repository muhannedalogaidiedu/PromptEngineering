{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210d2d0c",
   "metadata": {},
   "source": [
    "# Prompt Engineering Playbook â€” Colab Notebook\n",
    "\n",
    "This notebook contains **20 prompting techniques** with runnable Python examples.\n",
    "Use the first code cell to select your **model provider** (Gemini, OpenAI, Anthropic, Mistral, Cohere, etc.).\n",
    "\n",
    "**Tips**\n",
    "- For review workflows, set a lower temperature (e.g., `0.1â€“0.3`) for more deterministic outputs.\n",
    "- Avoid adding PHI/PII unless your environment is compliant.\n",
    "- For RAG and ReAct, replace the toy stubs with your real tools/APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd9599",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“˜ Table of Techniques (Clickable & Summarized)\n",
    "\n",
    "| # | Technique (click to jump) | Summary |\n",
    "|---|----------------------------|----------|\n",
    "| 1 | [Zero-shot](#1-zero-shot-prompting-no-examples-instruction-only) | No examples; direct instruction |\n",
    "| 2 | [One-shot](#2-one-shot-prompting-one-example-guides-output) | One example provided to guide style |\n",
    "| 3 | [Few-shot](#3-few-shot-prompting-several-examples-teach-the-pattern) | Multiple examples to infer task pattern |\n",
    "| 4 | [Chain-of-Thought](#4-chain-of-thought-cot-step-by-step-reasoning) | Step-by-step reasoning to improve logic |\n",
    "| 5 | [Self-Consistency](#5-self-consistency-sample-multiple-paths-and-compare) | Multiple reasoning paths â†’ consistent result |\n",
    "| 6 | [ReAct](#6-react-reason--act--decide-call-a-tool-then-answer) | Combine reasoning + tool use |\n",
    "| 7 | [Tree-of-Thought](#7-tree-of-thought--branch-and-evaluate-reasoning-paths) | Explore reasoning branches; pick best |\n",
    "| 8 | [Generated Knowledge](#8-generated-knowledge--create-background-then-answer) | Generate context first, then solve |\n",
    "| 9 | [RAG](#9-retrieval-augmented-generation-rag--toy-retrieval-stub) | Retrieve external docs before answering |\n",
    "| 10 | [Instruction](#10-instruction-prompting--explicit-structured-directive) | Direct, rule-based task prompt |\n",
    "| 11 | [Contextual](#11-contextual-prompting--add-situational-background) | Add domain or role context |\n",
    "| 12 | [Role](#12-role-prompting--assign-a-persona-for-toneauthority) | Act as a domain expert |\n",
    "| 13 | [CoT Explicit Steps](#13-cot-with-explicit-steps--numbered-phases) | Structured numbered reasoning |\n",
    "| 14 | [Multi-turn](#14-multi-turn-prompting--keep-brief-history-across-turns) | Multi-round conversation w/ memory |\n",
    "| 15 | [Program-Aided](#15-program-aided-prompting-pap--combine-code--llm) | Combine computation + language model |\n",
    "| 16 | [Least-to-Most](#16-least-to-most--solve-easy-subproblems-first) | Break down complex â†’ simple parts |\n",
    "| 17 | [Meta](#17-meta-prompting--prompts-about-prompts) | Improve prompts through reflection |\n",
    "| 18 | [Automatic Prompt Engineering](#18-automatic-prompt-engineering-ape--toy-candidate-scoring) | Auto-generate optimized prompts |\n",
    "| 19 | [Multimodal](#19-multimodal-prompting--placeholder-for-text--image-models) | Combine text + images |\n",
    "| 20 | [Contrastive](#20-contrastive-prompting--compare-alternatives-explicitly) | Compare perspectives for balance |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================[ Provider Setup ]=========================\n",
    "# Choose ONE provider below (uncomment its block) and set your API key.\n",
    "# The helper `llm(prompt, model=..., temperature=...)` keeps the rest\n",
    "# of the notebook vendor-agnostic.\n",
    "#\n",
    "# If nothing is configured, calling `llm(...)` will raise a helpful error.\n",
    "# ===================================================================\n",
    "\n",
    "TEMPERATURE = 0.2\n",
    "\n",
    "# 1) Google Gemini (google-generativeai)\n",
    "# !pip -q install google-generativeai\n",
    "# import google.generativeai as genai\n",
    "# import os\n",
    "# genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\"))\n",
    "# def llm(prompt, model=\"gemini-1.5-pro\", temperature=TEMPERATURE):\n",
    "#     return genai.GenerativeModel(model).generate_content(\n",
    "#         prompt,\n",
    "#         generation_config={\"temperature\": temperature}\n",
    "#     ).text\n",
    "\n",
    "# 2) OpenAI\n",
    "# !pip -q install openai\n",
    "# from openai import OpenAI\n",
    "# import os\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "# def llm(prompt, model=\"gpt-4.1-mini\", temperature=TEMPERATURE):\n",
    "#     r = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         temperature=temperature,\n",
    "#         messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "#     )\n",
    "#     return r.choices[0].message.content\n",
    "\n",
    "# 3) Anthropic Claude\n",
    "# !pip -q install anthropic\n",
    "# import anthropic, os\n",
    "# client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "# def llm(prompt, model=\"claude-3-5-sonnet-20241022\", temperature=TEMPERATURE):\n",
    "#     msg = client.messages.create(\n",
    "#         model=model,\n",
    "#         temperature=temperature,\n",
    "#         max_tokens=1200,\n",
    "#         messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    "#     )\n",
    "#     # Concatenate text blocks\n",
    "#     out = []\n",
    "#     for blk in msg.content:\n",
    "#         if getattr(blk, \"type\", \"\") == \"text\":\n",
    "#             out.append(blk.text)\n",
    "#     return \"\\n\".join(out) if out else str(msg)\n",
    "\n",
    "# 4) Mistral\n",
    "# !pip -q install mistralai\n",
    "# from mistralai.client import MistralClient\n",
    "# import os\n",
    "# client = MistralClient(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "# def llm(prompt, model=\"mistral-large-latest\", temperature=TEMPERATURE):\n",
    "#     r = client.chat(model=model, temperature=temperature, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "#     return r.choices[0].message.content\n",
    "\n",
    "# 5) Cohere\n",
    "# !pip -q install cohere\n",
    "# import cohere, os\n",
    "# co = cohere.Client(api_key=os.getenv(\"COHERE_API_KEY\"))\n",
    "# def llm(prompt, model=\"command-r-plus\", temperature=TEMPERATURE):\n",
    "#     r = co.chat(model=model, temperature=temperature, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "#     return r.text\n",
    "\n",
    "def not_configured():\n",
    "    raise RuntimeError(\"Please configure ONE provider in the setup cell above (uncomment a block and add your API key/env var).\")\n",
    "\n",
    "try:\n",
    "    llm  # type: ignore\n",
    "except NameError:\n",
    "    def llm(prompt, model=None, temperature=TEMPERATURE):\n",
    "        return not_configured()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d769e",
   "metadata": {},
   "source": [
    "> **Batching:** For bulk reviews, wrap calls in loops and store outputs (CSV/DB).\n",
    "\"\n",
    "\"> **Guardrails:** When requiring JSON, validate with a schema and retry on failure.\n",
    "\"\n",
    "\"> **RAG:** Replace toy retrieval with FAISS/Chroma/pgvector, and add citations in outputs.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe56b5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1) Zero-shot prompting â€” No examples; instruction only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Summarize this quarterly financial risk report into 5 bullets, plain English:\n",
    "[PASTE REPORT TEXT HERE]\"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d3063",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2) One-shot prompting â€” One example guides output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a824ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"Example summary style:\n",
    "- Key risk drivers: liquidity, FX exposure\n",
    "- Material changes: inventory +12% QoQ\n",
    "- Action items: hedge EUR, tighten DSO\n",
    "\"\"\"\n",
    "prompt = example + \"\\nNow summarize the new report similarly:\\n[NEW REPORT]\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdc20d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Few-shot prompting â€” Several examples teach the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = \"\"\"\n",
    "Task: Classify transaction as FRAUD or OK.\n",
    "\n",
    "Example 1:\n",
    "Input: Merchant=ABC Travel, Amount=$4,920, Country=US\n",
    "Label: OK\n",
    "\n",
    "Example 2:\n",
    "Input: Merchant=CryptoX, Amount=$9,990, Country=RU\n",
    "Label: FRAUD\n",
    "\"\"\"\n",
    "prompt = shots + \"\\nPredict label for:\\nInput: Merchant=XYZ GiftCards, Amount=$7,500, Country=Unknown\\nLabel:\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4941bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) Chain-of-Thought (CoT) â€” Step-by-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edacf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Analyze these ratios and conclude company health.\n",
    "Think step-by-step (show brief reasoning, then final verdict):\n",
    "\n",
    "Data:\n",
    "- Current ratio 1.9\n",
    "- Debt/Equity 0.6\n",
    "- Gross margin 42%\n",
    "\n",
    "Output: Reasoning (brief) -> Verdict\n",
    "\"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b8b34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) Self-consistency â€” Sample multiple paths and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ca815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "prompt = \"Given symptoms: fever, rash, joint painâ€”list top likely diagnoses (3).\"\n",
    "votes = collections.Counter(llm(prompt) for _ in range(3))\n",
    "print(\"Consensus:\", votes.most_common(1)[0][0])\n",
    "print(\"\\nAll candidates:\\n\", \"\\n---\\n\".join(votes.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced858ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6) ReAct (Reason + Act) â€” Decide, call a tool, then answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d685c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_db(query:str)->str:\n",
    "    # Replace with your real DB/search function\n",
    "    return \"DB_RESULT: Latest EPS $1.22, YoY +8%.\"\n",
    "\n",
    "plan = llm(\"\"\"You can REASON then ACT with tools.\n",
    "Question: \"Is the company's EPS trending up?\"\n",
    "Think: We should fetch EPS.\n",
    "Act: search_db(\"EPS trend for ACME\")\"\"\")\n",
    "\n",
    "tool_result = search_db(\"EPS trend for ACME\")\n",
    "final = llm(f\"Context from tool: {tool_result}\\nAnswer the original question succinctly.\")\n",
    "\n",
    "print(\"Plan:\\n\", plan, \"\\n\")\n",
    "print(\"Tool result:\\n\", tool_result, \"\\n\")\n",
    "print(\"Final:\\n\", final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5089be4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7) Tree-of-Thought â€” Branch and evaluate reasoning paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ceb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Choose best portfolio: {A: 70/30}, {B: 50/50}, {C: 30/70} for a 5-yr horizon.\"\n",
    "branches = [llm(f\"Option {opt}: pros/cons, risk, return, suitability for moderate-risk investor.\") for opt in [\"A\",\"B\",\"C\"]]\n",
    "judge = llm(\"Evaluate the following analyses and pick the best for a moderate-risk investor:\\n\" + \"\\n\\n\".join(branches))\n",
    "print(\"=== Branches ===\\n\" + \"\\n\\n\".join(branches))\n",
    "print(\"\\n=== Verdict ===\\n\" + judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9442a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8) Generated Knowledge â€” Create background, then answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a03487",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge = llm(\"Summarize company ACME: products, markets, risks, last 2 years trends.\")\n",
    "answer = llm(f\"Using this background:\\n{knowledge}\\nNow: Assess ACMEâ€™s liquidity risks in 5 bullets.\")\n",
    "print(\"Background:\\n\", knowledge, \"\\n\")\n",
    "print(\"Assessment:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a770bee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9) Retrieval-Augmented Generation (RAG) â€” Toy retrieval stub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba775ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = [\n",
    "    \"10-K excerpt: Operating cash flow up 12%, CAPEX stable.\",\n",
    "    \"Earnings call: Guidance raised for FY, FX headwinds easing.\"\n",
    "]\n",
    "prompt = f\"Ground your answer ONLY in these docs:\\n{retrieved[0]}\\n{retrieved[1]}\\n\\nQuestion: Summarize growth drivers.\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e26fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10) Instruction prompting â€” Explicit, structured directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Extract all dollar amounts > $10,000 and return JSON array of numbers only.\n",
    "Text:\n",
    "- Purchase: $4,500\n",
    "- Equipment: $25,000\n",
    "- Settlement: $180,000\n",
    "- Fees: $9,900\n",
    "\"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ce874",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11) Contextual prompting â€” Add situational background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Context: You are a senior medical reviewer.\n",
    "Task: Turn these notes into a concise assessment & plan (<=120 words).\n",
    "Notes: [PASTE CLINICAL NOTES]\n",
    "\"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e48ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12) Role prompting â€” Assign a persona for tone/authority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a compliance officer. Identify high-risk clauses in this NDA and explain why in 3 bullets:\n",
    "[PASTE NDA]\n",
    "\"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d07793",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13) CoT with Explicit Steps â€” Numbered phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Follow these steps:\n",
    "1) Read contract text\n",
    "2) Extract obligations per party\n",
    "3) List 3 risks with brief rationale\n",
    "Contract:\n",
    "[PASTE CONTRACT]\n",
    "Output format:\n",
    "- Obligations: {Party A:[], Party B:[]}\n",
    "- Risks: [..]\n",
    "\"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053b3d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14) Multi-turn prompting â€” Keep brief history across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "def chat(user):\n",
    "    history.append({\"role\":\"user\",\"content\":user})\n",
    "    response = llm(\"Conversation:\\n\"+str(history)+\"\\nAssistant:\")\n",
    "    history.append({\"role\":\"assistant\",\"content\":response})\n",
    "    return response\n",
    "\n",
    "print(chat(\"We saw revenue up 12%. Do we have regional breakdown?\"))\n",
    "print(chat(\"Yes, EMEA led. Summarize next steps.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797cdc5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15) Program-Aided Prompting (PAP) â€” Combine code + LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0216fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = {\"current\":1.9, \"quick\":1.5, \"de_ratio\":0.6}\n",
    "prompt = f\"Given ratios = {ratios}, provide a concise health assessment (<=80 words).\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8279400",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16) Least-to-Most â€” Solve easy subproblems first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy = llm(\"Compute revenue growth: 120M -> 138M YoY. Give % growth only.\")\n",
    "hard = llm(f\"Given growth={easy}, discuss sustainability in 4 bullets (drivers, risks, outlook, watchlist).\")\n",
    "print(\"Easy step:\\n\", easy, \"\\n\")\n",
    "print(\"Hard step:\\n\", hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4def3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 17) Meta prompting â€” Prompts about prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"I need a better prompt to extract â€˜change of controlâ€™ clauses reliably.\n",
    "Suggest 3 improved prompts with rationale, and one evaluation metric to compare them.\n",
    "\"\"\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d0767",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 18) Automatic Prompt Engineering (APE) â€” Toy candidate scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"Extract change-of-control clauses. Return standardized JSON with clause text and risk level.\",\n",
    "    \"Identify change-of-control clauses. Add span indices and a one-line rationale.\",\n",
    "    \"Find ALL change-of-control clauses; output CSV fields: start,end,text,risk(score 1-5).\"\n",
    "]\n",
    "scores = []\n",
    "for p in candidates:\n",
    "    grade = llm(f\"Rate this prompt for recall & precision (0-10 each), then give total:\\n{p}\")\n",
    "    scores.append((p, grade))\n",
    "print(\"=== Candidates & Grades ===\")\n",
    "for i,(p,g) in enumerate(scores, 1):\n",
    "    print(f\"{i}) {p}\\nGrade:\\n{g}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbad2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 19) Multimodal prompting â€” Placeholder for text + image models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real environment, pass text + image to a multimodal model (e.g., Gemini, GPT-4o, Claude Vision).\n",
    "# Example (pseudo):\n",
    "# from PIL import Image\n",
    "# img = Image.open(\"/content/xray.png\")\n",
    "# prompt = \"Given this chest X-ray, list 3 notable findings with caveats.\"\n",
    "# response = multimodal_model.generate([prompt, img])\n",
    "print(\"[Multimodal placeholder] Provide both text + image to a multimodal model's SDK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ac85a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 20) Contrastive prompting â€” Compare alternatives explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Compare two treatments (A: ACE inhibitor, B: ARB) for hypertension.\n",
    "Provide: efficacy, side effects, cost, guideline stance. End with a balanced recommendation.\"\"\"\n",
    "print(llm(prompt))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
