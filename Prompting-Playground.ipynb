{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3796e69a",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª Prompting Playground â€” LangChain (Handsâ€‘On)\n",
    "\n",
    "**Last generated:** 2025-11-01 03:13:37\n",
    "\n",
    "This notebook is a *hands-on* tour of practical prompting patterns using LangChain:\n",
    "1. Basic `PromptTemplate`\n",
    "2. `ChatPromptTemplate` (system + user messages)\n",
    "3. `FewShotPromptTemplate`\n",
    "4. Structured outputs with `PydanticOutputParser` (JSON)\n",
    "5. RAG prompt patterns with `create_stuff_documents_chain`\n",
    "6. (Optional) Mapâ€‘Reduce style document chain\n",
    "7. Exercises\n",
    "\n",
    "> ðŸ’¡ Tip: Run this notebook in **Google Colab** or a local venv with Python 3.10+.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519144d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… Setup (Uncomment if needed)\n",
    "# If you're on Colab or a fresh environment, install the dependencies:\n",
    "# !pip -q install -U langchain langchain-openai langchain-community pydantic pydantic-core tiktoken\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Put your key here or set it in your environment before running ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # <- Prefer setting it in env, not here.\n",
    "\n",
    "# Imports (LangChain 0.2+ / 0.3+ style)\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Optional: documents & chains\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Utility for pretty printing\n",
    "from pprint import pprint\n",
    "\n",
    "# Create an LLM handle (adjust model to what you have access to)\n",
    "# If you don't have gpt-4o, try \"gpt-4o-mini\" or \"gpt-4.1-mini\" or \"gpt-4.1\"\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=os.environ.get(\"OPENAI_API_KEY\", None))\n",
    "str_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e817cb4",
   "metadata": {},
   "source": [
    "## 1) Basic `PromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d74425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A simple single-message template (non-chat) for direct formatting\n",
    "basic_prompt = PromptTemplate.from_template(\n",
    "    \"Explain the following concept in simple terms for a beginner:\\n\\nTopic: {topic}\"\n",
    ")\n",
    "\n",
    "# 1) Inspect the formatted text (no model call yet)\n",
    "formatted = basic_prompt.format(topic=\"Neural Networks\")\n",
    "print(\"---- Rendered Prompt ----\\n\", formatted)\n",
    "\n",
    "# 2) Connect to LLM using LCEL pipe: prompt | llm | parser\n",
    "basic_chain = basic_prompt | llm | str_parser\n",
    "\n",
    "# 3) Invoke the chain\n",
    "resp = basic_chain.invoke({\"topic\": \"Neural Networks\"})\n",
    "print(\"\\n---- LLM Response ----\\n\", resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1098e",
   "metadata": {},
   "source": [
    "## 2) `ChatPromptTemplate` (system + user messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952210b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a patient and friendly tutor. Respond concisely.\"),\n",
    "    (\"human\", \"Please explain {topic} with a simple metaphor and one bullet list of 3 key points.\")\n",
    "])\n",
    "\n",
    "# View the rendered message list\n",
    "rendered = chat_prompt.invoke({\"topic\": \"Gradient Descent\"})\n",
    "print(\"---- Rendered Chat Messages ----\")\n",
    "for m in rendered.to_messages():\n",
    "    print(m.type.upper() + \":\", m.content)\n",
    "\n",
    "# Build chain\n",
    "chat_chain = chat_prompt | llm | str_parser\n",
    "print(\"\\n---- LLM Response ----\\n\", chat_chain.invoke({\"topic\": \"Gradient Descent\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d7a5a",
   "metadata": {},
   "source": [
    "## 3) `FewShotPromptTemplate` (examples to steer the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8367b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provide small labeled examples\n",
    "examples = [\n",
    "    {\"term\": \"Overfitting\", \"explanation\": \"Model memorizes training data and performs poorly on new data.\"},\n",
    "    {\"term\": \"Regularization\", \"explanation\": \"A technique to discourage overly complex models and reduce overfitting.\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Term: {term}\\nShort explanation: {explanation}\")\n",
    "\n",
    "fewshot = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Now explain the term briefly:\\nTerm: {query}\",\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "print(\"---- Rendered Few-Shot Prompt ----\\n\")\n",
    "print(fewshot.format(query=\"Cross-Validation\"))\n",
    "\n",
    "fewshot_chain = fewshot | llm | str_parser\n",
    "print(\"\\n---- LLM Response ----\\n\", fewshot_chain.invoke({\"query\": \"Cross-Validation\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fb73d",
   "metadata": {},
   "source": [
    "## 4) Structured JSON output with `PydanticOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e14c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a schema for the response\n",
    "class Concept(BaseModel):\n",
    "    term: str = Field(description=\"The technical term being defined\")\n",
    "    definition: str = Field(description=\"A concise definition in plain English\")\n",
    "    difficulty: int = Field(description=\"Difficulty from 1 (easy) to 5 (hard)\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Concept)\n",
    "\n",
    "schema_instructions = parser.get_format_instructions()\n",
    "\n",
    "structured_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a precise assistant that MUST output valid JSON matching the given schema.\"),\n",
    "    (\"human\", \"Term: {term}\\n\\nReturn JSON matching this schema:\\n{format_instructions}\")\n",
    "]).partial(format_instructions=schema_instructions)\n",
    "\n",
    "structured_chain = structured_prompt | llm | parser\n",
    "\n",
    "result = structured_chain.invoke({\"term\": \"Batch Normalization\"})\n",
    "print(\"---- Parsed Pydantic Object ----\")\n",
    "print(result)\n",
    "print(\"\\nAs dict:\")\n",
    "print(result.dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601dccf",
   "metadata": {},
   "source": [
    "## 5) RAG prompt pattern â€” `create_stuff_documents_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecdccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build an instruction that accepts a {context} (documents) and a {question}\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a careful assistant. Answer the question using ONLY the provided context.\\n\"\n",
    "    \"If the answer is not in the context, say \\\"I don't know.\\\"\\n\\n\"\n",
    "    \"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "# Fake documents (you would use your retriever's results in practice)\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain is a framework for building with LLMs. It helps with prompts, chains, and retrieval.\"),\n",
    "    Document(page_content=\"The create_stuff_documents_chain function injects document text into a single prompt for the LLM.\"),\n",
    "]\n",
    "\n",
    "# Build a document chain that will render {context} from docs and pass to the LLM\n",
    "doc_chain = create_stuff_documents_chain(llm, rag_prompt)\n",
    "\n",
    "# ðŸ‘‰ How to see the final rendered prompt?\n",
    "# The chain itself manages rendering, but we can emulate what it does by joining doc texts.\n",
    "context_text = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "rendered_rag = rag_prompt.format(context=context_text, question=\"What is LangChain used for?\")\n",
    "\n",
    "print(\"---- Rendered RAG Prompt (preview) ----\\n\")\n",
    "print(rendered_rag)\n",
    "\n",
    "# Now invoke the actual chain (it will do the stuffing internally)\n",
    "answer = doc_chain.invoke({\"question\": \"What is LangChain used for?\", \"context\": docs})\n",
    "print(\"\\n---- LLM Answer ----\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2dc872",
   "metadata": {},
   "source": [
    "## 6) (Optional) Mapâ€‘Reduce document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248069a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapâ€‘Reduce is useful for lots of long documents.\n",
    "# In LangChain 0.2/0.3, the high-level factory for map-reduce may vary by version.\n",
    "# Below is a pattern using two prompts and a manual reduce step (kept simple).\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the following chunk concisely:\\n\\n{chunk}\"\n",
    ")\n",
    "reduce_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Combine the following partial summaries into one crisp summary:\\n\\n{summaries}\"\n",
    ")\n",
    "\n",
    "chunks = [\n",
    "    \"Chunk A: LangChain supports prompts, LLMs, agents, and tools.\",\n",
    "    \"Chunk B: Retrieval helps the model ground answers in external knowledge.\",\n",
    "    \"Chunk C: Output parsers enforce structure such as JSON or Pydantic models.\"\n",
    "]\n",
    "\n",
    "# Map stage\n",
    "map_chain = map_prompt | llm | str_parser\n",
    "partials = [map_chain.invoke({\"chunk\": c}) for c in chunks]\n",
    "\n",
    "# Reduce stage\n",
    "reduce_chain = reduce_prompt | llm | str_parser\n",
    "final_summary = reduce_chain.invoke({\"summaries\": \"\\n\\n\".join(partials)})\n",
    "\n",
    "print(\"---- Map-Reduce Partial Summaries ----\")\n",
    "for i, p in enumerate(partials, 1):\n",
    "    print(f\"[{i}] {p}\\n\")\n",
    "print(\"---- Final Combined Summary ----\\n\", final_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd8d41",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Challenges / Exercises\n",
    "\n",
    "1. **Guardrail**: Modify the structured prompt to include a `sources: List[str]` field and make the model cite anything it claims.\n",
    "2. **Style Transfer**: Use `ChatPromptTemplate` to rewrite a paragraph in three different tones (formal, friendly, technical).\n",
    "3. **RAG Swap**: Replace the fake `docs` with a real vectorstore retriever pipeline and feed the retrieved docs into the stuff chain.\n",
    "4. **JSON Mode**: Try `JsonOutputParser` with a custom JSON schema (no Pydantic) and validate the result.\n",
    "5. **Long Context**: Split a long PDF into chunks â†’ map-summarize â†’ reduce â†’ answer a question.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
