# ChatGPT Prompt Engineering for Developers ğŸš€

## What Youâ€™ll Learn ğŸ¯

* **Prompt engineering best practices** for application development.
* **New ways to use LLMs**â€”including building your own custom chatbot.
* **Hands-on practice** writing and iterating prompts with the OpenAI API.

---

## About This Course ğŸ§ 

In **ChatGPT Prompt Engineering for Developers**, youâ€™ll learn how to harness large language models (LLMs) to build powerful apps quickly. Using the OpenAI API, you can implement features that were previously too costly, too technical, or downright impossible. Taught by **Isa Fulford (OpenAI)** and **Andrew Ng (DeepLearning.AI)**, the course explains how LLMs work, shares **prompt engineering best practices**, and demonstrates practical applications for:

* **Summarizing** (e.g., condensing user reviews) âœ‚ï¸
* **Inferring** (e.g., sentiment, topics) ğŸ§­
* **Transforming** text (e.g., translation, grammar correction, format conversion) ğŸ”
* **Expanding** content (e.g., drafting emails) âœï¸

Youâ€™ll also learn two core principles for effective prompting, how to **iterate prompts systematically**, and how to **build a custom chatbot**. The concepts are illustrated with examples you can run in a Jupyter notebook for hands-on practice. ğŸ’»

---

## Introduction ğŸŒŸ

Welcome! Andrew Ng introduces Isa Fulford, who co-built the ChatGPT Retrieval plugin, contributes to the **OpenAI Cookbook**, and teaches best practices for using LLMs in products. While many guides focus on one-off prompts in the ChatGPT UI, this course emphasizes **LLMs as developer tools via APIs** for building real applications fast.

Youâ€™ll explore:

1. **Prompting best practices** for software development.
2. **Common use cases**â€”summarizing, inferring, transforming, expanding.
3. **Building a chatbot** using an LLM.

### Base vs. Instruction-Tuned LLMs ğŸ§©

* **Base LLMs** predict the next token from vast web text. They may continue your input (â€œOnce upon a timeâ€¦â€) but might respond with unrelated question lists if you ask factual queries.
* **Instruction-tuned LLMs** are fine-tuned to **follow instructions** and often further refined with **RLHF** (Reinforcement Learning from Human Feedback) to be **helpful, honest, and harmless**. For most practical apps, the course recommends using **instruction-tuned** models.

**Mindset tip:** Treat prompting like giving instructions to a smart colleague who doesnâ€™t know your task context yet. Be explicit about **what** you want (focus, tone, length, references) and **how** to handle inputs.

---

## Guidelines: Two Core Principles ğŸ§­

### 1) Write **Clear and Specific** Instructions âœï¸

Longer prompts can be **clearer**, not worse.

**Tactics:**

* **Use delimiters** to isolate user input or resources

  * Triple backticks, quotes, XML tags, section titles.
  * Helps avoid **prompt injection** (â€œignore previous instructionsâ€¦â€ stays inside the delimited text to be summarized, not executed).
* **Ask for structured output** ğŸ“¦

  * Request **JSON/HTML** with specific keys (e.g., `{"book_id": "...", "title": "...", "author": "...", "genre": "..."}`) to simplify parsing.
* **Check preconditions** âœ…

  * Instruct the model to verify assumptions first and return a fallback like **â€œNo steps provided.â€** if they arenâ€™t met.
* **Few-shot prompting** ğŸ§ª

  * Show examples of **desired style** or **format** (e.g., a grandparentâ€™s tone) and then ask for a new answer â€œin a consistent style.â€

### 2) Give the Model **Time to Think** ğŸ•°ï¸

If tasks require reasoning, **ask for intermediate steps** or a **chain of reasoning** *before* a final answer.

**Tactics:**

* **Specify step-by-step actions** (e.g., summarize â†’ translate â†’ extract names â†’ output JSON).
* **Constrain the output format** with simple labeled sections to make parsing robust.
* **Have the model solve first, then compare** ğŸ§®

  * For grading a studentâ€™s solution, instruct: *â€œWork out your own solution, then compare; only then grade.â€* This reduces reasoning mistakes.

**Limitation & Mitigation:**
LLMs can **hallucinate**. Reduce risk by requiring **grounded quotes** first and then answering **from those quotes**. Add traceability whenever possible. ğŸ”

---

## Iterative Prompt Development ğŸ”

Like ML model training, you **wonâ€™t get the perfect prompt on the first try**. Use a loop:
**Idea â†’ First Prompt â†’ Result â†’ Error Analysis â†’ Refine â†’ Repeat.**

**Example:** Marketing description from a **technical fact sheet** for a chair.

* First pass: too long â†’ add **length constraint** (words/sentences/characters).
* Change **audience** (retailers vs. consumers) to emphasize **technical details**.
* Append requirements like **â€œinclude every 7-character product ID.â€**
* Eventually add **rendered HTML** and a **dimension table**.

Mature apps may evaluate prompts on many examples (10â€“100+) for **average/worst-case performance**. Early on, single examples + quick iterations are fine. âš™ï¸

---

## Summarizing ğŸ—ï¸â¡ï¸âœ‚ï¸

Use LLMs to **digest long text** (e.g., product reviews) quickly.

**Patterns:**

* **General summary** with clear **length controls** (words/sentences/characters).
* **Audience-specific** summaries (e.g., **shipping** vs. **pricing** department).
* **Extraction instead of summary** when you only need one fact (e.g., â€œWhat happened with delivery?â€).
* **Batch processing** loop: iterate over many reviews and print concise summaries for dashboards.

---

## Inferring ğŸ”

Treat inference as **analysis** over text: **sentiment**, **emotions**, **anger detection**, **entity extraction**, **topic tagging**.

**Examples:**

* **Binary sentiment** (â€œpositiveâ€/â€œnegativeâ€) for easy downstream logic.
* **Emotions list** (â‰¤5 items).
* **Anger detection** (boolean for escalation workflows).
* **Information extraction**: item purchased, brand, etc., **as JSON**.
* **Multi-label topics**: provide a topic list and ask for **0/1 per topic** (prefer JSON for robustness).

This replaces multiple supervised models with **a single promptable API**â€”huge speed boost for development. âš¡

---

## Transforming ğŸ”„

LLMs can convert across **languages, tones, and formats**â€”and fix **grammar/spelling**.

**Translation** ğŸŒ

* Identify language.
* Translate to one or many languages (even playful ones like â€œEnglish Pirateâ€).
* Handle **formal vs. informal** variants (e.g., Spanish *usted* vs. *tÃº*).
* Build a **universal translator** pipeline for multilingual IT support.

**Tone transformation** ğŸ­

* Convert slang to a **business letter**, or adjust for **audience** and **register**.

**Format conversion** ğŸ—‚ï¸

* JSON â†’ HTML table (or any structured format).
* Prefer explicit **input/output schemas** to reduce variance.

**Proofread & Rewrite** âœ¨

* Correct grammar and spelling.
* Add constraints: **â€œIf no errors, say â€˜No errors found.â€™â€**
* Use diff tools (e.g., *redlines*) to highlight changes.
* Apply style guides (e.g., **APA**), increase **compelling tone**, and return **Markdown**.

---

## Building a Custom Chatbot ğŸ¤–

With the same prompting principles, assemble a **chatbot** that:

* Follows **clear system instructions** (role, boundaries, voice).
* Uses **structured outputs** for UI components.
* Reduces hallucinations by **retrieval** (quotes first, answer second).
* Iterates prompts with **evaluation sets** for stability.

---

## Conclusion ğŸ‰

You now have a practical toolkit:

* **Two principles:** be **clear & specific**, and **give time to think**.
* **A process:** **iterate** prompts like experiments.
* **Capabilities:** **summarizing**, **inferring**, **transforming**, **expanding**, and **chatbot building**.

Start small and have funâ€”use your early projects to build better ones next. These tools are powerful: **use them responsibly** and aim for positive impact. ğŸŒ±

Happy buildingâ€”and share what you create! âœ¨
